{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite model optimization\n",
    "\n",
    "TensorFlow Lite에서는 edge device에서 모델의 inference를 효율적으로 하기 위한 방법인 quantization과 pruning을 이용할 수 있는 toolkit을 제공하고 있다. 이번 실습에서는 해당 toolkit을 이용해 볼 것이다.\n",
    "\n",
    "더 자세한 내용은 document를 참고\n",
    "- https://www.tensorflow.org/model_optimization/guide\n",
    "- https://www.tensorflow.org/lite/performance/model_optimization\n",
    "- TFLite https://www.tensorflow.org/lite/guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What is TFLite?\n",
    "\n",
    "TensorFlow Lite는 TensorFlow로 학습된 모델을 tflite이라는 포맷으로 바꿔 줌으로써 모바일 장치, 임베디드 장치 등의 가벼운 장치에서 모델을 inference 할 수 있게 해주는 플랫폼이다.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/115159215-fccd0800-a0cc-11eb-817b-645f96c76966.png\" width=\"400\" height=\"400\"/>  \n",
    "\n",
    "\n",
    "TensorFlow는 모델을 파일로 저장할 때는 [Protocol Buffer](https://developers.google.com/protocol-buffers)라는 포맷으로 저장을 하고, TensorFlow Lite의 모델은 [FlatBuffer](https://google.github.io/flatbuffers/)라는 포맷으로 저장을 한다. 두 serialization의 가장 큰 차이점은 FlatBuffer는 추가적인 메모리 할당 없이 deserialize를 할 수 있기 때문에 빠르게 파일에 있는 데이터에 접근할 수 있다는 점이다.\n",
    "\n",
    "TFLite 모델로 변환을 할 때, quantization과 pruning 등의 optimization을 할 수 있는데, 이 때의 장점은 다음과 같다.\n",
    "- <b>Size reduction</b>\n",
    "  - smaller storage size, smaller download size, less memory usage\n",
    "  \n",
    "  \n",
    "- <b>Latency reduction</b>\n",
    "  - computation을 줄여 줌으로써 inference에 걸리는 시간 감소\n",
    "  \n",
    "  \n",
    "- <b>Accelerator compatibility</b>\n",
    "  - Edge TPU와 같이 특정 hardware는 각각의 quantization 요구 사항을 만족해야 함 (e.g. Coral Dev Board를 사용하려면 quantization aware training이나 full integer post-training quantization을 필요로 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow 최신 버전 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y tensorflow\n",
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "assert float(tf.__version__[:3]) >= 2.3\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "### 1. [Post-training quantization](https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html)\n",
    "\n",
    "<b>Post-training quantization</b>은 학습을 한 다음에 quantization을 하는 방법이다. Quantization aware training보다는 accuracy가 좀 떨어질 수 있지만, 학습된 모델을 가져와서 쉽게 쓸 수 있다. Tensorflow에서는 다음과 같은 post-quantization 기법들을 제공하고 있다.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114367164-2232b100-9bb7-11eb-81e7-b9b8dec09de5.png\" width=\"700\" height=\"700\"/>  \n",
    "- dynamic range quantization: weight은 ***8 bit integer***로 저장하고 inference 시에는 ***8 bit float***로 바뀌고, activation은 range에 따라서 ***(dynamic) 8 bit float***으로 저장\n",
    "\n",
    "\n",
    "나중에 Edge TPU가 있는 Coral board를 사용할 것이기 때문에 학습된 모델을 작동하는데 필요한 post-training integer quantization을 다뤄 볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 먼저 quantization을 할 간단한 모델을 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4717 - accuracy: 0.8311 - val_loss: 0.3753 - val_accuracy: 0.8644\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3262 - accuracy: 0.8855 - val_loss: 0.3209 - val_accuracy: 0.8829\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2883 - accuracy: 0.8975 - val_loss: 0.3139 - val_accuracy: 0.8857\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2655 - accuracy: 0.9045 - val_loss: 0.3051 - val_accuracy: 0.8908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf9f582640>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the input image\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,  \n",
    "  epochs=4,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 학습된 모델 Tensorflow Lite 모델로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 변환하기 위해서는 [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert)라는 것을 이용해야 합니다.\n",
    "다음과 같이 tflite 모델로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환할 때 아무런 옵션을 주지 않았으므로 이 모델은 단순히 full precision 32-bit float tflite 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Post-training integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114906428-f9c7e280-9e54-11eb-9038-9854d5c8d741.png\" width=\"700\" height=\"700\"/>  \n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114907026-9e4a2480-9e55-11eb-980f-55999688c260.png\" width=\"500\" height=\"500\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 셀을 실행하면 quantization이 완료 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100): #인퍼런스할 데이터와 같은 형태의 샘플 데이터\n",
    "                                                                                            #with rmin and rmax, calculate zero and scale\n",
    "        yield [input_value]\n",
    "\n",
    "post_train_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "post_train_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "post_train_converter.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "post_train_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] #Int8 only quantization\n",
    "\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "post_train_converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "post_train_converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "post_quant_tflite_model = post_train_converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 Quantized된 모델의 성능 평가를 위한 helper function ```evaluate_model(interpreter)```이다. Input type을 integer로 했으므로, input에 대한 quantization을 먼저 진행하고 변환된 데이터셋으로 inference를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    global test_images\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    input_index = input_details[\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # 테스트 데이터셋 전체에 대해서 예측\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if (i+1) % 1000 == 0:\n",
    "              print('Evaluated on {n} results so far.'.format(n=i+1))\n",
    "\n",
    "        # Input quantization\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        # Pre-processing: 배치 차원 추가\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # 인퍼런스\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: 배치 차원 삭제 및 prediction_digits에 예측값 추가\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "        \n",
    "    # 예측값과 ground truth 레이블 비교를 통해 정확도 계산\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tflite 모델로 inference를 하려면 [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter)를 이용한다. `model_content` 대신에 `model_path`를 이용하면 tflite 파일을 불러올 수도 있다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference를 하기 위해서는 interpreter 생성 &#8594; `allocate_tensor` &#8594; `set_tensor` &#8594; `invoke` 순으로 실행한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "Evaluated on 10000 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "Evaluated on 10000 results so far.\n"
     ]
    }
   ],
   "source": [
    "# 32bit float tflite 모델로 interpreter 만듦\n",
    "interpreter = tf.lite.Interpreter(\n",
    "        model_content=tflite_model\n",
    "        #OR model_path=\"path_to_tflite\"\n",
    ")\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "tflite_test_accuracy, _ = evaluate_model(interpreter) # Quantization 안된 모델\n",
    "\n",
    "# 8bit integer tflite\n",
    "post_quant_interpreter = tf.lite.Interpreter(model_content=post_quant_tflite_model)\n",
    "post_quant_interpreter.allocate_tensors()\n",
    "post_quant_tflite_test_accuracy, output = evaluate_model(post_quant_interpreter) # Quantization 된 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreter의 `get_input_details`, `get_output_details` 함수를 통해 input과 output의 type 및 quantization 정보를 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32 bit full precision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'input_1',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 28, 28], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Identity',\n",
       "  'index': 22,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'input_1',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 28, 28], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.003921568859368563, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_quant_interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Identity',\n",
       "  'index': 24,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.00390625, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_quant_interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline 모델과 quantized 모델의 accuracy를 비교해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite test accuracy: 0.8908\n",
      "Post-training Quant TFLite test_accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "print('TFLite test accuracy:', tflite_test_accuracy)\n",
    "print('Post-training Quant TFLite test_accuracy:', post_quant_tflite_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 파일로 저장해서 실제로 모델 크기가 줄어들었는지 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.08170700073242188\n",
      "Quantized model in Mb: 0.02512359619140625\n"
     ]
    }
   ],
   "source": [
    "# create temp file\n",
    "_, tflite_file = tempfile.mkstemp('.tflite')\n",
    "_, post_quant_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "\n",
    "with open(tflite_file, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "with open(post_quant_tflite_file, 'wb') as f:\n",
    "    f.write(post_quant_tflite_model)\n",
    "\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(tflite_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(post_quant_tflite_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 : TensorFlow에서 제공하는 pretrain model 가져와서 post-training quantization 해보기\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/densenet\"><code translate=\"no\" dir=\"ltr\">densenet</code></a> module: Public API for tf.keras.applications.densenet namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/efficientnet\"><code translate=\"no\" dir=\"ltr\">efficientnet</code></a> module: Public API for tf.keras.applications.efficientnet namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils\"><code translate=\"no\" dir=\"ltr\">imagenet_utils</code></a> module: Public API for tf.keras.applications.imagenet_utils namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2\"><code translate=\"no\" dir=\"ltr\">inception_resnet_v2</code></a> module: Public API for tf.keras.applications.inception_resnet_v2 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3\"><code translate=\"no\" dir=\"ltr\">inception_v3</code></a> module: Public API for tf.keras.applications.inception_v3 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet\"><code translate=\"no\" dir=\"ltr\">mobilenet</code></a> module: Public API for tf.keras.applications.mobilenet namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2\"><code translate=\"no\" dir=\"ltr\">mobilenet_v2</code></a> module: Public API for tf.keras.applications.mobilenet_v2 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3\"><code translate=\"no\" dir=\"ltr\">mobilenet_v3</code></a> module: Public API for tf.keras.applications.mobilenet_v3 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/nasnet\"><code translate=\"no\" dir=\"ltr\">nasnet</code></a> module: Public API for tf.keras.applications.nasnet namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet\"><code translate=\"no\" dir=\"ltr\">resnet</code></a> module: Public API for tf.keras.applications.resnet namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50\"><code translate=\"no\" dir=\"ltr\">resnet50</code></a> module: Public API for tf.keras.applications.resnet50 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2\"><code translate=\"no\" dir=\"ltr\">resnet_v2</code></a> module: Public API for tf.keras.applications.resnet_v2 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16\"><code translate=\"no\" dir=\"ltr\">vgg16</code></a> module: Public API for tf.keras.applications.vgg16 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg19\"><code translate=\"no\" dir=\"ltr\">vgg19</code></a> module: Public API for tf.keras.applications.vgg19 namespace.</p>\n",
    "\n",
    "<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/xception\"><code translate=\"no\" dir=\"ltr\">xception</code></a> module: Public API for tf.keras.applications.xception namespace.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain 모델 import 하기.\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트에는 다음의 test image를 이용할 것이다. quantization을 할 때 representative data로는 이 이미지 하나만 이용해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  6677  100  6677    0     0  79488      0 --:--:-- --:--:-- --:--:-- 79488\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FAF802E3760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://cdn.education.com/files/526001_527000/526114/file_526114.jpg\"\n",
    "os.system(\"curl \" + url + \" > balloon.jpg\")\n",
    "img_path = 'balloon.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "display(img)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x) # 각 모델을 학습할 때 preprocess했던 방법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate 할 때는 다음 함수를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_practice(interpreter, test_images):\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    input_index = input_details[\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=(i+1)))\n",
    "\n",
    "        # quantize input\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        # Pre-processing: add batch dimension\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        preds = interpreter.tensor(output_index)\n",
    "        preds = np.expand_dims(preds()[0], axis=0)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/miniconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "/home/david/miniconda3/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1 results so far.\n",
      "Predicted: [('n02782093', 'balloon', 244), ('n04023962', 'punching_bag', 8), ('n03942813', 'ping-pong_ball', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 1. import한 pretrain 모델 가져오기\n",
    "# =================================TODO=======================================\n",
    "your_model = ResNet50(weights='imagenet')\n",
    "# =================================TODO Fin===================================\n",
    "# 2. full integer post-training quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(your_model)\n",
    "model_tflite = converter.convert() # 기존 모델 tflite\n",
    "\n",
    "import numpy as np\n",
    "def representative_data_gen():\n",
    "    yield [x] # test할 이미지 1개만 사용\n",
    "# =================================TODO=======================================\n",
    "post_quant_converter = tf.lite.TFLiteConverter.from_keras_model(your_model)\n",
    "\n",
    "post_quant_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "post_quant_converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "post_quant_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "post_quant_converter.inference_input_type = tf.uint8\n",
    "post_quant_converter.inference_output_type = tf.uint8\n",
    "\n",
    "post_quant_tflite = post_quant_converter.convert()\n",
    "# =================================TODO Fin===================================\n",
    "\n",
    "\n",
    "# 3. make interpreter and do inference\n",
    "# =================================TODO=======================================\n",
    "post_quant_interpreter = tf.lite.Interpreter(model_content=post_quant_tflite)\n",
    "\n",
    "post_quant_interpreter.allocate_tensors()\n",
    "preds = evaluate_model_practice(post_quant_interpreter, x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# =================================TODO Fin==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크기 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 97.43085861206055\n",
      "Quantized model in Mb: 25.079750061035156\n"
     ]
    }
   ],
   "source": [
    "# Measure sizes of models.\n",
    "_, model_tflite_file = tempfile.mkstemp('.tflite')\n",
    "_, post_quant_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "\n",
    "with open(model_tflite_file, 'wb') as f:\n",
    "    f.write(model_tflite)\n",
    "\n",
    "with open(post_quant_tflite_file, 'wb') as f:\n",
    "    f.write(post_quant_tflite)\n",
    "\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(model_tflite_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(post_quant_tflite_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. [Quantization Aware Training(QAT)](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html)\n",
    "\n",
    "수업 시간에 배웠듯이 QAT는 나중에 quantization을 할 것이라는 것을 가정하고 학습하는 것이다. scratch부터 학습해도 되지만, 이미 학습된 모델을 fine tuning 하는 것이 좋다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Quantize whole model\n",
    "\n",
    "Edge TPU와 같이 fully quantized 모델이 필요한 경우에는 전체 모델에 대해 QAT를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 학습한 float32모델로 QAT를 해볼 것이다.`tfmot.quantization.keras.quantize_model`함수를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 28, 28)            3         \n",
      "_________________________________________________________________\n",
      "quant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d (QuantizeWrappe (None, 28, 28, 16)        195       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 14, 14, 16)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_1 (QuantizeWrap (None, 14, 14, 32)        4707      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_1 (Quant (None, 7, 7, 32)          1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 1568)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 10)                15695     \n",
      "=================================================================\n",
      "Total params: 20,604\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 114\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer에 quant가 붙은 것을 확인할 수 있다. 그 다음으로는, data의 일부를 이용해서 fine tuning 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 184ms/step - loss: 0.2237 - accuracy: 0.9233 - val_loss: 0.2948 - val_accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faec9ccbee0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data 일부를 이용해서 fine tuning\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "quant_aware_model.fit(train_images_subset, train_labels_subset,\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 quantization을 하지 않았으므로 weight은 실수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.variable_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실수 weight이므로 QAT를 한다고 해서 accuracy가 떨어지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8907999992370605\n",
      "QAT test accuracy: 0.8982999920845032\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('QAT test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 QAT로 학습한 모델을 quantization 해보자. post-training quantization과 비슷하게 TFLiteConverter를 이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as reshape_layer_call_and_return_conditional_losses, reshape_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "\n",
    "QAT_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantization을 해도 accuracy가 떨어지지 않는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24633/2552567575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mQAT_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mQAT_test_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQAT_interpreter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Baseline test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_model_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24633/3538800336.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(interpreter)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 인퍼런스\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Post-processing: 배치 차원 삭제 및 prediction_digits에 예측값 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    873\u001b[0m     \"\"\"\n\u001b[1;32m    874\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "QAT_interpreter = tf.lite.Interpreter(model_content=QAT_tflite_model)\n",
    "QAT_interpreter.allocate_tensors()\n",
    "\n",
    "QAT_test_accuracy, output = evaluate_model(QAT_interpreter)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Post-training Quantized test accuracy:', post_quant_tflite_test_accuracy)\n",
    "print('Quantized QAT test accuracy:', QAT_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 크기가 1/4로 줄어든 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp file\n",
    "_, QAT_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(QAT_tflite_file, 'wb') as f:\n",
    "    f.write(QAT_tflite_model)\n",
    "\n",
    "print(\"Float tflite model in Mb:\", os.path.getsize(tflite_file) / float(2**20))\n",
    "print(\"Quantized QAT model in Mb:\", os.path.getsize(QAT_tflite_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Quantize some layers\n",
    "\n",
    "Edge TPU 등 특별한 하드웨어가 아닌 경우에는 선택적으로 layer를 quantization할 수도 있다.\n",
    "\n",
    "**Tips for better accuracy**\n",
    "* from scratch보다는 fine tuning\n",
    "* 뒤쪽 layer를 quantization 하기\n",
    "* 특별히 중요한 layer는 quantization 피하기 (e.g. attention layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-1. 마지막 레이어만 quantization하는 fine-tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer만 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.layers[-1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tfmot.quantization.keras.quantize_annotate_layer` 함수를 이용해서 특정 layer를 quantization할 것이라고 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def apply_quantization_to_dense(layer):\n",
    "    # if isinstance(layer, tf.keras.layers.Dense)\n",
    "    if \"dense\" in layer.name:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    else:\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.models.clone_model` 함수와 helper function을 이용해서 annotated model을 만들고, `tfmot.quantization.keras.quantize_apply`함수를 이용해서 QAT를 위한 모델을 만들면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization_to_dense,\n",
    ")\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2-2. From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10)) # Sequential 내부에 layer 추가\n",
    "                                                                              # 마지막 Dense만 quantization\n",
    "])\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model) # Quantization 진행 \n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functional example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "x = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(inputs)\n",
    "\n",
    "# 이 Conv2D layer만 quantization\n",
    "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'))(x) \n",
    "\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "annotated_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. [Pruning](https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html)\n",
    "\n",
    "Pruning은 불필요한 (i.e. 0에 가까운) weight을 0으로 만들어 모델이 해야하는 계산을 줄이면서 optimize를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tfmot.sparsity.keras.prune_low_magnitude`을 이용해서 모델을 만든다. 이 때, 논문에서 제안된 pruning 방식인 `tfmot.sparsity.keras.PolynomialDecay` 을 parameter로 넘겨준다. `PolynomialDecay`에는 다음과 같은 hyperparameter들이 있다.\n",
    "* `initial_sparsity`: pruning을 시작할 때의 sparsity를 몇으로 할 지\n",
    "* `final_sparsity`: pruning을 끝낼 때의 sparsity를 몇으로 할 지\n",
    "* `begin_step`: pruning을 언제부터 진행할 지 (batch 단위의 step)\n",
    "* `end_step`: pruning을 언제 끝낼 지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Pruning을 하기 전에 크기 비교시 사용하기 위해 baseline model을 미리 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Pruning을 위한 모델과 파라미터를 정의한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "num_images = train_images.shape[0]\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,#pruning을 시작할 때의 sparsity\n",
    "                                                               final_sparsity=0.80, #pruning을 끝낼 때의 sparsity\n",
    "                                                               begin_step=10, #언제 pruning을 시작할지 (배치 단위)\n",
    "                                                               end_step=end_step) #언제 pruning을 멈출지\n",
    "}\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. 학습\n",
    "* `fit`을 할 때, `tfmot.sparsity.keras.UpdatePruningStep`을 callback으로 불러야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "]\n",
    "  \n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. 결과 \n",
    "baseline 모델과 accuracy를 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-5. 변수 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 summary를 보면 pruning을 하기 위해 non-trainable parameter가 생긴 것을 알 수 있다. `tfmot.sparsity.keras.strip_pruning`을 이용해서 pruning에 사용한 variable을 제거해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning된 모델 tflite으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-6. 학습된 모델 크기 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruning을 해도 weight matrix의 크기는 그대로이다. 하지만 matrix의 값이 대부분 0이기 때문에 감소된 크기를 확인하려면 실제로 zip파일 등으로 압축하는 과정을 거쳐야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    return os.path.getsize(zipped_file)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f kB\" % (get_gzipped_model_size(keras_file))) #Baseline 모델 압축\n",
    "print(\"Size of gzipped pruned Keras model: %.2f kB\" % (get_gzipped_model_size(pruned_keras_file))) #Keras 모델 압축 \n",
    "print(\"Size of gzipped pruned TFlite model: %.2f kB\" % (get_gzipped_model_size(pruned_tflite_file))) #TFLite 모델 압축"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pruning 과 Quantization 합치기\n",
    "pruning을 한 다음에 post-training quantization까지 하면 크기를 더 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export) # 위에서 사용한 pruning된 모델 \n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "    f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. baseline 모델과 accuracy 비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy, _ = evaluate_model(interpreter)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAT와 비슷하게, pruning도 layer를 선택적으로 pruning 할 수 있다. 이 부분은 [TF Pruning guide](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)를 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix. [ONNX](https://github.com/onnx/onnx)\n",
    "* Reference:  https://github.com/omerferhatt/torch2tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 Tensorflow 모델을 이용해서 tflite 모델을 만들고 싶은데, Tensorflow 모델은 없고 Pytorch 모델만 있다면...? 이럴 때 사용하는 것이 ONNX이다. ONNX는 서로다른 deep learning framework에서 모델을 사용하고 싶을 때, 모델을 원하는 framework로 변환해 주는 기능을 제공한다. \n",
    "  \n",
    "즉, ONNX를 이용해서 pytorch 모델을 tensorflow 모델로 바꾼 다음, tensorflow lite 모델을 만들어 주면 된다.\n",
    "  \n",
    "* 모든 모델을 완벽하게 바꿔주는 것은 아니여서 (e.g. accuracy 감소 및 변환이 안되는 operation 등이 있을 수 있음), 처음부터 학습해야 하는 경우에는 pytorch 코드를 tensorflow 코드로 구현해서 모델을 학습하는게 좋고, 그게 잘 안되거나 pretrained pytorch 모델을 꼭 써야하는 경우에 이용\n",
    "\n",
    "* Tensorflow, Pytorch, ONNX 세 개의 플랫폼의 버전 호환성으로 인해 실제로 본인이 원하는 모델을 바꾸려면 많은 시행착오가 필요함.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow -y\n",
    "!pip uninstall torch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-nightly\n",
    "!pip install tensorflow-addons==0.11.2\n",
    "!pip install torch==1.7.0\n",
    "!pip install onnx==1.8.0\n",
    "!pip install onnx-tf==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnx\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from PIL import Image\n",
    "from onnx_tf.backend import prepare\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 함수들 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_input(image_file):\n",
    "    \"\"\"\n",
    "    Loads image from disk and converts to compatible shape.\n",
    "    :param image_file: Path to single image file\n",
    "    :return: Original image, numpy.ndarray instance image, torch.Tensor image\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    image = cv2.imread(image_file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(image)\n",
    "    torch_img = transform(pil_img)\n",
    "    torch_img = torch_img.unsqueeze(0)\n",
    "    torch_img = torch_img.to(torch.device(\"cpu\"))\n",
    "\n",
    "    return image, torch_img.numpy(), torch_img\n",
    "def torch_to_onnx(torch_path, onnx_path, image_path):\n",
    "    \"\"\"\n",
    "    Converts PyTorch model file to ONNX with usable op-set\n",
    "    :param torch_path: Torch model path to load\n",
    "    :param onnx_path: ONNX model path to save\n",
    "    :param image_path: Path of test image to use in export progress\n",
    "    \"\"\"\n",
    "    pytorch_model = get_torch_model(torch_path)\n",
    "    image, tf_lite_image, torch_image = get_example_input(image_path)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model=pytorch_model,\n",
    "        args=torch_image,\n",
    "        f=onnx_path,\n",
    "        verbose=False,\n",
    "        export_params=True,\n",
    "        do_constant_folding=False,  # fold constant values for optimization\n",
    "        input_names=['input'],\n",
    "        opset_version=10,\n",
    "        output_names=['output'])\n",
    "def tf_to_tf_lite(tf_path, tf_lite_path):\n",
    "    \"\"\"\n",
    "    Converts TF saved model into TFLite model\n",
    "    :param tf_path: TF saved model path to load\n",
    "    :param tf_lite_path: TFLite model path to save\n",
    "    \"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)  # Path to the SavedModel directory\n",
    "    tflite_model = converter.convert()  # Creates converter instance\n",
    "    with open(tf_lite_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "def get_torch_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads state-dict into model and creates an instance\n",
    "    :param model_path: State-dict path to load PyTorch model with pre-trained weights\n",
    "    :return: PyTorch model instance\n",
    "    \"\"\"\n",
    "    model = torch.load(model_path, map_location='cpu')\n",
    "    return model\n",
    "def get_tf_lite_model(model_path):\n",
    "    \"\"\"\n",
    "    Creates an instance of TFLite CPU interpreter\n",
    "    :param model_path: TFLite model path to initialize\n",
    "    :return: TFLite interpreter\n",
    "    \"\"\"\n",
    "    interpret = tf.lite.Interpreter(model_path)\n",
    "    interpret.allocate_tensors()\n",
    "    return interpret\n",
    "def predict_tf_lite(model, image):\n",
    "    \"\"\"\n",
    "    TFLite model prediction (forward propagate)\n",
    "    :param model: TFLite interpreter\n",
    "    :param image: Input image\n",
    "    :return: Numpy array with logits\n",
    "    \"\"\"\n",
    "    input_details = model.get_input_details()\n",
    "    output_details = model.get_output_details()\n",
    "    model.set_tensor(input_details[0]['index'], image)\n",
    "    model.invoke()\n",
    "    tf_lite_output = model.get_tensor(output_details[0]['index'])\n",
    "    return tf_lite_output\n",
    "def predict_torch(model, image):\n",
    "    \"\"\"\n",
    "    Torch model prediction (forward propagate)\n",
    "    :param model: PyTorch model\n",
    "    :param image: Input image\n",
    "    :return: Numpy array with logits\n",
    "    \"\"\"\n",
    "    return model(image).data.cpu().numpy()\n",
    "def convert(torch_model_path, tf_lite_model_path, image_path):\n",
    "    if os.path.exists('output'):\n",
    "        shutil.rmtree('output')\n",
    "        os.mkdir('output')\n",
    "    else:\n",
    "        os.mkdir('output')\n",
    "    ONNX_PATH = \"output/onnx_model.onnx\"\n",
    "    TF_PATH = \"output/tf_model\"\n",
    "\n",
    "    try:\n",
    "        torch_to_onnx(torch_path=torch_model_path, onnx_path=ONNX_PATH, image_path=image_path)\n",
    "        print('\\n\\nTorch to ONNX converted!\\n\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        onnx_to_tf(onnx_path=ONNX_PATH, tf_path=TF_PATH)\n",
    "        print('\\n\\nONNX to TF converted!\\n\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        tf_to_tf_lite(tf_path=TF_PATH, tf_lite_path=tf_lite_model_path)\n",
    "        print('\\n\\nTF to TFLite converted!\\n\\n')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "def calc_error(res1, res2, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculates specified error between two results. In here Mean-Square-Error and Mean-Absolute-Error calculated\"\n",
    "    :param res1: First result\n",
    "    :param res2: Second result\n",
    "    :param verbose: Print loss results\n",
    "    :return: Loss metrics as a dictionary\n",
    "    \"\"\"\n",
    "    mse = ((res1 - res2) ** 2).mean(axis=None)\n",
    "    mae = np.abs(res1 - res2).mean(axis=None)\n",
    "    metrics = {'mse': mse, 'mae': mae}\n",
    "    if verbose:\n",
    "        print(f\"Mean-Square-Error between predictions: {metrics['mse']}\")\n",
    "        print(f\"Mean-Absolute-Error between predictions: {metrics['mae']}\\n\")\n",
    "    return metrics     \n",
    "\n",
    "def init_models(torch_model_path, tf_lite_model_path):\n",
    "    \"\"\"\n",
    "    Initialize the Torch and TFLite models\n",
    "    :param torch_model_path: Path to Torch model\n",
    "    :param tf_lite_model_path: Path to TFLite model\n",
    "    :return: CPU initialized models\n",
    "    \"\"\"\n",
    "    torch_model = get_torch_model(torch_model_path)\n",
    "    tf_lite_model = get_tf_lite_model(tf_lite_model_path)\n",
    "    return torch_model, tf_lite_model\n",
    "\n",
    "def onnx_to_tf(onnx_path, tf_path):\n",
    "    \"\"\"\n",
    "    Converts ONNX model to TF 2.X saved file\n",
    "    :param onnx_path: ONNX model path to load\n",
    "    :param tf_path: TF path to save\n",
    "    \"\"\"\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(onnx_model)  # Checks signature\n",
    "    tf_rep = prepare(onnx_model)  # Prepare TF representation\n",
    "    tf_rep.export_graph(tf_path)  # Export the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX 변환 예시를 위한 Pytorch 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "torch.save(resnet50, 'torch_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from IPython.display import display\n",
    "\n",
    "url = \"https://cdn.education.com/files/526001_527000/526114/file_526114.jpg\"\n",
    "os.system(\"curl \" + url + \" > balloon.jpg\")\n",
    "img_path = 'balloon.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(torch_model_path='torch_model.pt',\n",
    "        tf_lite_model_path='tflite_model.tflite',\n",
    "        image_path=img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image, tf_lite_image, torch_image = get_example_input(img_path)\n",
    "torch_model, tf_lite_model = init_models('torch_model.pt', 'tflite_model.tflite')\n",
    "\n",
    "tf_lite_output = predict_tf_lite(tf_lite_model, tf_lite_image)\n",
    "torch_output = predict_torch(torch_model, torch_image)\n",
    "# Calculates loss metrics of outputs between two model\n",
    "_ = calc_error(tf_lite_output, torch_output, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import decode_predictions\n",
    "print('Predicted:', decode_predictions(tf_lite_output, top=3)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
